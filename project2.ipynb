{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2-COVID-19 Open Research Dataset Challenge (CORD-19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset description\n",
    "In response to the COVID-19 pandemic, the White House and a coalition of leading research groups have prepared the COVID-19 Open Research Dataset (CORD-19). CORD-19 is a resource of over 500,000 scholarly articles, including over 200,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. This freely available dataset is provided to the global research community to apply recent advances in natural language processing and other AI techniques to generate new insights in support of the ongoing fight against this infectious disease. There is a growing urgency for these approaches because of the rapid acceleration in new coronavirus literature, making it difficult for the medical research community to keep up.\n",
    "\n",
    "### In this project we are required to do the following tasks:\n",
    "1. Collect and process pdf data dump from COVID-19 Open Research Dataset Challenge (CORD-19)\n",
    "2.  Analyze the data and provide publication statistics such as the number of publications according to time, location but not limited to. Provide (any type of) visualization for the results.\n",
    "3. Using sentence embedding from the articles' abstract and main content respectively.\n",
    "4. Build a tool for question answering: given a user input sentence or query, outputs the top 10 most relevant sentences from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:37:34.219592Z",
     "iopub.status.busy": "2021-10-31T11:37:34.219171Z",
     "iopub.status.idle": "2021-10-31T11:37:43.455976Z",
     "shell.execute_reply": "2021-10-31T11:37:43.454544Z",
     "shell.execute_reply.started": "2021-10-31T11:37:34.219542Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install word2number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:37:43.459561Z",
     "iopub.status.busy": "2021-10-31T11:37:43.459146Z",
     "iopub.status.idle": "2021-10-31T11:37:52.345027Z",
     "shell.execute_reply": "2021-10-31T11:37:52.344086Z",
     "shell.execute_reply.started": "2021-10-31T11:37:43.459505Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:37:52.347221Z",
     "iopub.status.busy": "2021-10-31T11:37:52.346951Z",
     "iopub.status.idle": "2021-10-31T11:38:01.126540Z",
     "shell.execute_reply": "2021-10-31T11:38:01.125328Z",
     "shell.execute_reply.started": "2021-10-31T11:37:52.347185Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:38:01.129602Z",
     "iopub.status.busy": "2021-10-31T11:38:01.129308Z",
     "iopub.status.idle": "2021-10-31T11:38:01.267951Z",
     "shell.execute_reply": "2021-10-31T11:38:01.266768Z",
     "shell.execute_reply.started": "2021-10-31T11:38:01.129565Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import pycountry\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import unidecode\n",
    "from word2number import w2n\n",
    "import contractions\n",
    "\n",
    "from langdetect import detect\n",
    "from langdetect import DetectorFactory\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "import gensim\n",
    "from nltk import tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and exploring dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:38:01.270003Z",
     "iopub.status.busy": "2021-10-31T11:38:01.269484Z",
     "iopub.status.idle": "2021-10-31T11:38:01.729305Z",
     "shell.execute_reply": "2021-10-31T11:38:01.727788Z",
     "shell.execute_reply.started": "2021-10-31T11:38:01.269956Z"
    }
   },
   "outputs": [],
   "source": [
    "#importing pdf_json dataset\n",
    "path = '../input/CORD-19-research-challenge/document_parses/pdf_json'\n",
    "filenames = os.listdir(path)\n",
    "print(\"Number of articles in the dataset: \", len(filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for formating author column of pdf_json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:38:01.731405Z",
     "iopub.status.busy": "2021-10-31T11:38:01.731034Z",
     "iopub.status.idle": "2021-10-31T11:38:01.744108Z",
     "shell.execute_reply": "2021-10-31T11:38:01.742877Z",
     "shell.execute_reply.started": "2021-10-31T11:38:01.731372Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_names(author):\n",
    "    middle_names = \" \".join(author['middle'])\n",
    "    \n",
    "    if author['middle']:\n",
    "        return \" \".join([author['first'], middle_names, author['last']])\n",
    "    else:\n",
    "        return \" \".join([author['first'], author['last']])\n",
    "\n",
    "\n",
    "def format_affiliations(affiliations):\n",
    "    texts = []\n",
    "    location = affiliations.get('location')\n",
    "    if location:\n",
    "        texts.extend(list(affiliations['location'].values()))\n",
    "    \n",
    "    institution = affiliations.get('institution')\n",
    "    if institution:\n",
    "        texts = [institution] + texts\n",
    "    return \", \".join(texts)\n",
    "\n",
    "def format_author(authors, with_affiliation=False):\n",
    "    name_s = []\n",
    "    \n",
    "    for author in authors:\n",
    "        name = format_names(author)\n",
    "        if with_affiliation:\n",
    "            affiliations = format_affiliations(author['affiliation'])\n",
    "            if affiliations:\n",
    "                name_s.append(f\"{name} ({affiliations})\")\n",
    "            else:\n",
    "                name_s.append(name)\n",
    "        else:\n",
    "            name_s.append(name)\n",
    "    \n",
    "    return \", \".join(name_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:38:01.746984Z",
     "iopub.status.busy": "2021-10-31T11:38:01.746577Z",
     "iopub.status.idle": "2021-10-31T11:39:25.087516Z",
     "shell.execute_reply": "2021-10-31T11:39:25.086358Z",
     "shell.execute_reply.started": "2021-10-31T11:38:01.746931Z"
    }
   },
   "outputs": [],
   "source": [
    "counts = 0\n",
    "\n",
    "doc = []\n",
    "for file in tqdm(os.listdir(path)):\n",
    "    f_path = f\"{path}/{file}\"\n",
    "    j = json.load(open(f_path,\"rb\"))\n",
    "    paper_id = j['paper_id']\n",
    "    \n",
    "    paper_id = paper_id[:]\n",
    "    title = j['metadata']['title']\n",
    "    \n",
    "    authors = format_author(j['metadata']['authors'], \n",
    "                           with_affiliation=True)\n",
    "\n",
    "    try: \n",
    "        abstract = j['abstract'][0]['text']\n",
    "    except:\n",
    "        abstract = \"\"\n",
    "\n",
    "    full_text = \"\"\n",
    "    \n",
    "    \n",
    "    for t in j['body_text']:\n",
    "        full_text += t['text']\n",
    "\n",
    "    doc.append([paper_id, title, authors, abstract, full_text])\n",
    "\n",
    "    counts += 1\n",
    "\n",
    "    if (counts >= 10000) :\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making dataframe for the extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:39:25.089068Z",
     "iopub.status.busy": "2021-10-31T11:39:25.088793Z",
     "iopub.status.idle": "2021-10-31T11:39:25.117468Z",
     "shell.execute_reply": "2021-10-31T11:39:25.116294Z",
     "shell.execute_reply.started": "2021-10-31T11:39:25.089036Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(doc,columns=[\"paper_id\", \"title\", \"authors\", \"abstract\", \"full_text\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading metadata.csv and merging both dataframes into 1 based on paper ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:39:25.119290Z",
     "iopub.status.busy": "2021-10-31T11:39:25.118953Z",
     "iopub.status.idle": "2021-10-31T11:39:40.116454Z",
     "shell.execute_reply": "2021-10-31T11:39:40.115479Z",
     "shell.execute_reply.started": "2021-10-31T11:39:25.119243Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/CORD-19-research-challenge/metadata.csv', usecols=[\"pdf_json_files\", \"publish_time\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the id using str.extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:39:40.122176Z",
     "iopub.status.busy": "2021-10-31T11:39:40.120505Z",
     "iopub.status.idle": "2021-10-31T11:39:41.697834Z",
     "shell.execute_reply": "2021-10-31T11:39:41.696674Z",
     "shell.execute_reply.started": "2021-10-31T11:39:40.122117Z"
    }
   },
   "outputs": [],
   "source": [
    "df['paper_id'] = df['pdf_json_files'].str.extract(r'/\\w+/(\\w+)')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:39:41.699725Z",
     "iopub.status.busy": "2021-10-31T11:39:41.699491Z",
     "iopub.status.idle": "2021-10-31T11:39:42.272212Z",
     "shell.execute_reply": "2021-10-31T11:39:42.270885Z",
     "shell.execute_reply.started": "2021-10-31T11:39:41.699697Z"
    }
   },
   "outputs": [],
   "source": [
    "#merged data based on ID\n",
    "final_data = pd.merge(data,df,on='paper_id')\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### working on Publish_time column to visualise based on time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:39:42.274318Z",
     "iopub.status.busy": "2021-10-31T11:39:42.273978Z",
     "iopub.status.idle": "2021-10-31T11:39:42.305038Z",
     "shell.execute_reply": "2021-10-31T11:39:42.301065Z",
     "shell.execute_reply.started": "2021-10-31T11:39:42.274274Z"
    }
   },
   "outputs": [],
   "source": [
    "print (final_data['publish_time'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:39:42.309608Z",
     "iopub.status.busy": "2021-10-31T11:39:42.309203Z",
     "iopub.status.idle": "2021-10-31T11:39:42.357112Z",
     "shell.execute_reply": "2021-10-31T11:39:42.356247Z",
     "shell.execute_reply.started": "2021-10-31T11:39:42.309561Z"
    }
   },
   "outputs": [],
   "source": [
    "# extracting data for month and year\n",
    "final_data['publish_time'] =pd.to_datetime(final_data['publish_time'])\n",
    "print(final_data['publish_time'].dtype)\n",
    "final_data['year']=final_data['publish_time'].dt.year\n",
    "final_data['month']=final_data['publish_time'].dt.month\n",
    "final_data['date']=final_data['publish_time'].dt.date\n",
    "\n",
    "#printing final_data head\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:39:42.359343Z",
     "iopub.status.busy": "2021-10-31T11:39:42.359011Z",
     "iopub.status.idle": "2021-10-31T11:39:42.651369Z",
     "shell.execute_reply": "2021-10-31T11:39:42.650214Z",
     "shell.execute_reply.started": "2021-10-31T11:39:42.359301Z"
    }
   },
   "outputs": [],
   "source": [
    "final_data['year'].value_counts()[:6].plot(kind='barh', color ='brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:39:42.653848Z",
     "iopub.status.busy": "2021-10-31T11:39:42.653259Z",
     "iopub.status.idle": "2021-10-31T11:39:42.792109Z",
     "shell.execute_reply": "2021-10-31T11:39:42.788283Z",
     "shell.execute_reply.started": "2021-10-31T11:39:42.653795Z"
    }
   },
   "outputs": [],
   "source": [
    "final_data.year.value_counts()[:6].plot.pie()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualisation by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:39:42.795068Z",
     "iopub.status.busy": "2021-10-31T11:39:42.794553Z",
     "iopub.status.idle": "2021-10-31T11:39:43.087663Z",
     "shell.execute_reply": "2021-10-31T11:39:43.086403Z",
     "shell.execute_reply.started": "2021-10-31T11:39:42.795011Z"
    }
   },
   "outputs": [],
   "source": [
    "final_data['month'].value_counts()[:12].plot(kind='barh', color ='brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:39:43.089803Z",
     "iopub.status.busy": "2021-10-31T11:39:43.089531Z",
     "iopub.status.idle": "2021-10-31T11:39:43.256708Z",
     "shell.execute_reply": "2021-10-31T11:39:43.255927Z",
     "shell.execute_reply.started": "2021-10-31T11:39:43.089769Z"
    }
   },
   "outputs": [],
   "source": [
    "final_data.month.value_counts()[:12].plot.pie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:39:43.258923Z",
     "iopub.status.busy": "2021-10-31T11:39:43.258608Z",
     "iopub.status.idle": "2021-10-31T11:39:43.620660Z",
     "shell.execute_reply": "2021-10-31T11:39:43.619908Z",
     "shell.execute_reply.started": "2021-10-31T11:39:43.258886Z"
    }
   },
   "outputs": [],
   "source": [
    "#number of publications by date\n",
    "final_data['date'].value_counts()[:20].plot(kind='barh', color ='brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Author publications "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:39:43.622204Z",
     "iopub.status.busy": "2021-10-31T11:39:43.621831Z",
     "iopub.status.idle": "2021-10-31T11:39:43.837579Z",
     "shell.execute_reply": "2021-10-31T11:39:43.836674Z",
     "shell.execute_reply.started": "2021-10-31T11:39:43.622172Z"
    }
   },
   "outputs": [],
   "source": [
    "final_data['authors'].value_counts()[:1].plot(kind='barh', color ='brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:39:43.840113Z",
     "iopub.status.busy": "2021-10-31T11:39:43.839092Z",
     "iopub.status.idle": "2021-10-31T11:39:43.981767Z",
     "shell.execute_reply": "2021-10-31T11:39:43.980500Z",
     "shell.execute_reply.started": "2021-10-31T11:39:43.840056Z"
    }
   },
   "outputs": [],
   "source": [
    "final_data.authors.value_counts()[:2].plot.pie()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top author publication is more than 780 as the above pie chart shows the name of most popular authors which wrote 780+ articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the dataframe full_text and abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowercase all texts for body and abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:39:43.984307Z",
     "iopub.status.busy": "2021-10-31T11:39:43.983260Z",
     "iopub.status.idle": "2021-10-31T11:39:46.134220Z",
     "shell.execute_reply": "2021-10-31T11:39:46.132982Z",
     "shell.execute_reply.started": "2021-10-31T11:39:43.984270Z"
    }
   },
   "outputs": [],
   "source": [
    "final_data[\"full_text\"] = final_data[\"full_text\"].str.lower()\n",
    "final_data[\"abstract\"] = final_data[\"abstract\"].str.lower()\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sorting articles which only talk about covid using few covid related keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:39:46.135966Z",
     "iopub.status.busy": "2021-10-31T11:39:46.135509Z",
     "iopub.status.idle": "2021-10-31T11:39:46.920321Z",
     "shell.execute_reply": "2021-10-31T11:39:46.919268Z",
     "shell.execute_reply.started": "2021-10-31T11:39:46.135933Z"
    }
   },
   "outputs": [],
   "source": [
    "topics = ['asia','wuhan','covid-19','covid','covid19','corona','coronavirus','corona-virus','SARS','SARSCOV2','severe acute resperatory syndrom']\n",
    "\n",
    "label = []\n",
    "\n",
    "for a in tqdm(final_data[\"full_text\"]):\n",
    "    if any(x in a for x in topics):\n",
    "        label.append(1)\n",
    "    else :\n",
    "        label.append(0)\n",
    "        \n",
    "final_data['label'] = label\n",
    "\n",
    "final_data.drop(final_data.index[final_data['label']==0], inplace = True)\n",
    "\n",
    "\n",
    "len(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcloud before cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:39:46.922000Z",
     "iopub.status.busy": "2021-10-31T11:39:46.921741Z",
     "iopub.status.idle": "2021-10-31T11:42:14.283236Z",
     "shell.execute_reply": "2021-10-31T11:42:14.280831Z",
     "shell.execute_reply.started": "2021-10-31T11:39:46.921969Z"
    }
   },
   "outputs": [],
   "source": [
    "long = ','.join(list(final_data['full_text'].values))\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"black\", max_words=500, contour_width=5, contour_color='blue')\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long)\n",
    "\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing the non-english papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:42:14.285362Z",
     "iopub.status.busy": "2021-10-31T11:42:14.285112Z",
     "iopub.status.idle": "2021-10-31T11:49:31.912441Z",
     "shell.execute_reply": "2021-10-31T11:49:31.911426Z",
     "shell.execute_reply.started": "2021-10-31T11:42:14.285331Z"
    }
   },
   "outputs": [],
   "source": [
    "for a in tqdm(final_data['full_text']):\n",
    "    try:\n",
    "        if detect(a) != \"en\":\n",
    "            final_data.drop(final_data.index[final_data['full_text']==a], inplace = True)\n",
    "    except:\n",
    "        final_data.drop(final_data.index[final_data['full_text']==a], inplace = True)\n",
    "\n",
    "\n",
    "len(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:49:31.914825Z",
     "iopub.status.busy": "2021-10-31T11:49:31.914459Z",
     "iopub.status.idle": "2021-10-31T11:49:31.934474Z",
     "shell.execute_reply": "2021-10-31T11:49:31.933054Z",
     "shell.execute_reply.started": "2021-10-31T11:49:31.914775Z"
    }
   },
   "outputs": [],
   "source": [
    "final_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the paper which have length less than 200 of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:49:31.937019Z",
     "iopub.status.busy": "2021-10-31T11:49:31.936644Z",
     "iopub.status.idle": "2021-10-31T11:49:34.095028Z",
     "shell.execute_reply": "2021-10-31T11:49:34.093956Z",
     "shell.execute_reply.started": "2021-10-31T11:49:31.936966Z"
    }
   },
   "outputs": [],
   "source": [
    "final_data[\"split_text\"] = final_data[\"full_text\"].apply(lambda phrase: len(phrase.strip().split()))\n",
    "final_data.drop(final_data.index[final_data['split_text'] <= 200], inplace = True)\n",
    "len(final_data)\n",
    "#final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second part of cleaning using stopwords and punctutation removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using Stopwords using nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:49:34.097422Z",
     "iopub.status.busy": "2021-10-31T11:49:34.097084Z",
     "iopub.status.idle": "2021-10-31T11:50:48.755258Z",
     "shell.execute_reply": "2021-10-31T11:50:48.754144Z",
     "shell.execute_reply.started": "2021-10-31T11:49:34.097378Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(['reports','additions','india','usa','tables','papers','review','common','review','describes','abstract','retrospective','chart','patients','study',\n",
    "                        'associated','result','features','including','found','one','well','among','abstract','provide',\n",
    "                        'objective','background','range','feature','participates', 'copyright', 'many',\n",
    "                        'org', 'https', 'author', 'figure', 'table', 'rights', 'reserved', 'figures', 'reported',\n",
    "                        'permission', 'use', 'used', 'license','editor', 'brazil', 'article', 'figures', 'tables', \"the\", 'a', 'all', 'thus',\n",
    "                        'pubmed', 'editors', 'authors', 'methods', 'method', 'result', 'paper', 'introduction', 'editor', \n",
    "                         'although', 'letters', 'review', 'paper', 'table', 'addition', 'example', 'even', 'within', 'report']\n",
    "                        )\n",
    "final_data['full_text'] = final_data['full_text'].apply(lambda x: \" \".join([word for word in x.split() if word not in (stopwords)]))\n",
    "final_data['abstract'] = final_data['abstract'].apply(lambda x: \" \".join([word for word in x.split() if word not in (stopwords)]))\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:50:48.761967Z",
     "iopub.status.busy": "2021-10-31T11:50:48.761665Z",
     "iopub.status.idle": "2021-10-31T11:50:57.877251Z",
     "shell.execute_reply": "2021-10-31T11:50:57.875978Z",
     "shell.execute_reply.started": "2021-10-31T11:50:48.761932Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "final_data['full_text'] = final_data['full_text'].apply(lambda x: \" \".join(tokenizer.tokenize(x.lower())))\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:50:57.879578Z",
     "iopub.status.busy": "2021-10-31T11:50:57.879324Z",
     "iopub.status.idle": "2021-10-31T11:53:44.499027Z",
     "shell.execute_reply": "2021-10-31T11:53:44.498168Z",
     "shell.execute_reply.started": "2021-10-31T11:50:57.879549Z"
    }
   },
   "outputs": [],
   "source": [
    "long = ','.join(list(final_data['full_text'].values))\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"black\", max_words=500, contour_width=5, contour_color='blue')\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long)\n",
    "\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:53:44.501128Z",
     "iopub.status.busy": "2021-10-31T11:53:44.500618Z",
     "iopub.status.idle": "2021-10-31T11:53:44.509592Z",
     "shell.execute_reply": "2021-10-31T11:53:44.508349Z",
     "shell.execute_reply.started": "2021-10-31T11:53:44.501084Z"
    }
   },
   "outputs": [],
   "source": [
    "final_data.full_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:53:44.511574Z",
     "iopub.status.busy": "2021-10-31T11:53:44.511260Z",
     "iopub.status.idle": "2021-10-31T11:53:44.524430Z",
     "shell.execute_reply": "2021-10-31T11:53:44.523535Z",
     "shell.execute_reply.started": "2021-10-31T11:53:44.511543Z"
    }
   },
   "outputs": [],
   "source": [
    "# review =final_data.full_text.apply(gensim.utils.simple_preprocess)\n",
    "# review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:53:44.526084Z",
     "iopub.status.busy": "2021-10-31T11:53:44.525787Z",
     "iopub.status.idle": "2021-10-31T11:53:44.539285Z",
     "shell.execute_reply": "2021-10-31T11:53:44.537883Z",
     "shell.execute_reply.started": "2021-10-31T11:53:44.526049Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = gensim.models.Word2Vec(\n",
    "#     final_data['full_text'],\n",
    "#     window = 10,\n",
    "#     min_count =2,\n",
    "#     workers =10,\n",
    "#     epochs = 10\n",
    "    \n",
    "# )\n",
    "# model.build_vocab(final_data['full_text'], progress_per=1000)\n",
    "# model.corpus_count\n",
    "# model.train(final_data['full_text'], total_examples = model.corpus_count, epochs =30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 Sentence Embedding- using doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:53:44.542023Z",
     "iopub.status.busy": "2021-10-31T11:53:44.541522Z",
     "iopub.status.idle": "2021-10-31T11:53:44.556329Z",
     "shell.execute_reply": "2021-10-31T11:53:44.555342Z",
     "shell.execute_reply.started": "2021-10-31T11:53:44.541974Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_docs(data, tests=False):\n",
    "    \n",
    "    _documents = []\n",
    "    \n",
    "    if tests:\n",
    "        texts = data['full_text'].loc[0]\n",
    "        _documents.append((texts.lower(), 'no_tag'))\n",
    "    \n",
    "    else:\n",
    "        for rows in range(0, len(data)):\n",
    "            texts = data['full_text'].loc[rows]      \n",
    "            texts = texts.split('\\n\\n')\n",
    "            \n",
    "            publish_t = data['publish_time'].loc[rows]\n",
    "            authors = data['authors'].loc[rows]\n",
    "            title = data['title'].loc[rows]\n",
    "            \n",
    "            count_no=1\n",
    "\n",
    "            for a in texts:\n",
    "                a = a.lower()\n",
    "                if len(a)>=200:\n",
    "                    sentences = tokenize.sent_tokenize(a)\n",
    "                    if len(a)>100:\n",
    "                        tag_doc = ''.join([str(rows), '-', str(count_no)])\n",
    "                        \n",
    "                        _documents.append([a, tag_doc, publish_t, authors, title])\n",
    "                        count_no+=1\n",
    "                    else:\n",
    "                        pass\n",
    "                          \n",
    "    return _documents\n",
    "def _preprocess(docs, tokens_only=False):\n",
    "    for i, rec in enumerate(docs):\n",
    "        doc = rec[0]\n",
    "        tag_doc = rec[1]\n",
    "        tag_doc = ''.join([tag_doc, '_', str(i)])\n",
    "        tokens = gensim.utils.simple_preprocess(doc)\n",
    "        if tokens_only:\n",
    "            yield tokens\n",
    "        else:\n",
    "            yield gensim.models.doc2vec.TaggedDocument(tokens, [tag_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:53:44.559334Z",
     "iopub.status.busy": "2021-10-31T11:53:44.558122Z",
     "iopub.status.idle": "2021-10-31T11:55:16.496796Z",
     "shell.execute_reply": "2021-10-31T11:55:16.495455Z",
     "shell.execute_reply.started": "2021-10-31T11:53:44.559273Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = final_data[['full_text', 'publish_time', 'authors', 'title']]\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "train_docs = tokenize_docs(train_data)\n",
    "df_train = pd.DataFrame.from_records(train_docs, columns = ['document', 'tag_doc', 'publish_time', 'authors', 'title'])\n",
    "df_train.drop(columns=['document'], axis=1, inplace=True)\n",
    "\n",
    "print('Trained-Documents: ', len(train_docs))\n",
    "train_cor_pus = list(_preprocess(train_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:55:16.499470Z",
     "iopub.status.busy": "2021-10-31T11:55:16.498555Z",
     "iopub.status.idle": "2021-10-31T11:55:16.503016Z",
     "shell.execute_reply": "2021-10-31T11:55:16.502039Z",
     "shell.execute_reply.started": "2021-10-31T11:55:16.499431Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(train_cor_pus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Build -gensim doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:55:16.505194Z",
     "iopub.status.busy": "2021-10-31T11:55:16.504757Z",
     "iopub.status.idle": "2021-10-31T11:55:28.791766Z",
     "shell.execute_reply": "2021-10-31T11:55:28.791132Z",
     "shell.execute_reply.started": "2021-10-31T11:55:16.505151Z"
    }
   },
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=1, epochs=40)\n",
    "model.build_vocab(train_cor_pus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T11:55:28.793656Z",
     "iopub.status.busy": "2021-10-31T11:55:28.792941Z",
     "iopub.status.idle": "2021-10-31T12:13:05.070498Z",
     "shell.execute_reply": "2021-10-31T12:13:05.068958Z",
     "shell.execute_reply.started": "2021-10-31T11:55:28.793617Z"
    }
   },
   "outputs": [],
   "source": [
    "model.train(train_cor_pus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-4 Tool for user input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T12:13:05.072634Z",
     "iopub.status.busy": "2021-10-31T12:13:05.072357Z",
     "iopub.status.idle": "2021-10-31T12:13:05.086698Z",
     "shell.execute_reply": "2021-10-31T12:13:05.085648Z",
     "shell.execute_reply.started": "2021-10-31T12:13:05.072601Z"
    }
   },
   "outputs": [],
   "source": [
    "def user_answer(user_query, _results):\n",
    "    df_test = pd.DataFrame(data={'full_text':[user_query]})\n",
    "    test_docs = tokenize_docs(df_test, tests=True)\n",
    "    test_corpus = list(_preprocess(test_docs, tokens_only=True))\n",
    "\n",
    "    \n",
    "    i_vector = model.infer_vector(test_corpus[0])\n",
    "    sims = model.docvecs.most_similar([i_vector], topn=len(model.docvecs))\n",
    "\n",
    "    print('TEST Docs: «{}»\\n'.format(' '.join(test_corpus[0])))\n",
    "    print(u'Similar documents as per the sentence input using doc2vec model %s:\\n' % model)\n",
    "\n",
    "    results = [(f'TOP {i}', i) for i in range(0,_results)]\n",
    "    _papers = []\n",
    "    result = []\n",
    "    for labels, i in results:\n",
    "        print('Sentence Docs: «{}»\\n'.format(' '.join(test_corpus[0])))\n",
    "        doc_split = sims[i][0].split('_')\n",
    "        tag_doc = doc_split[0]\n",
    "        doc_index = int(doc_split[1])\n",
    "        print('Index of document from the dataset', doc_index)\n",
    "        _papers.append(int(doc_split[0].split('-')[0]))\n",
    "        extract = ' '.join(map(str, train_docs[doc_index]))\n",
    "        \n",
    "        print(u'%s %s:\\n%s\\n' % (labels, sims[i], extract))\n",
    "        print('-------')\n",
    "        print(extract)\n",
    "        result.append([tag_doc, extract])\n",
    "        \n",
    "        print('='*80)\n",
    "    \n",
    "    new_dataframe = pd.DataFrame().from_records(result, columns= ['tag_doc', 'extract'])\n",
    "    return new_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T12:13:05.089585Z",
     "iopub.status.busy": "2021-10-31T12:13:05.088480Z",
     "iopub.status.idle": "2021-10-31T12:13:20.571907Z",
     "shell.execute_reply": "2021-10-31T12:13:20.570627Z",
     "shell.execute_reply.started": "2021-10-31T12:13:05.089526Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Input the sentence or query')\n",
    "print('::::::----------------------::::::')\n",
    "user_input = input()\n",
    "print('::::::----------------------::::::')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T12:13:20.573895Z",
     "iopub.status.busy": "2021-10-31T12:13:20.573630Z",
     "iopub.status.idle": "2021-10-31T12:13:20.738437Z",
     "shell.execute_reply": "2021-10-31T12:13:20.737668Z",
     "shell.execute_reply.started": "2021-10-31T12:13:20.573837Z"
    }
   },
   "outputs": [],
   "source": [
    "dataframe_output = user_answer(user_input, 10)\n",
    "dataframe_output = dataframe_output.merge(df_train, how='left', on='tag_doc')\n",
    "dataframe_output = dataframe_output[['publish_time', 'authors', 'title', 'extract']]\n",
    "\n",
    "dataframe_output.to_csv('./result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokeniznig the abstract column of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T12:13:20.740649Z",
     "iopub.status.busy": "2021-10-31T12:13:20.740189Z",
     "iopub.status.idle": "2021-10-31T12:13:20.752636Z",
     "shell.execute_reply": "2021-10-31T12:13:20.751560Z",
     "shell.execute_reply.started": "2021-10-31T12:13:20.740615Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_docs(data, tests=False):\n",
    "    \n",
    "    _documents = []\n",
    "    \n",
    "    if tests:\n",
    "        texts = data['abstract'].loc[0]\n",
    "        _documents.append((texts.lower(), 'no_tag'))\n",
    "    \n",
    "    else:\n",
    "        for rows in range(0, len(data)):\n",
    "            texts = data['abstract'].loc[rows]      \n",
    "            texts = texts.split('\\n\\n')\n",
    "            \n",
    "            publish_t = data['publish_time'].loc[rows]\n",
    "            authors = data['authors'].loc[rows]\n",
    "            title = data['title'].loc[rows]\n",
    "            \n",
    "            count_no=1\n",
    "\n",
    "            for a in texts:\n",
    "                a = a.lower()\n",
    "                if len(a)>=300:\n",
    "                    sentences = tokenize.sent_tokenize(a)\n",
    "                    if len(a)>200:\n",
    "                        tag_doc = ''.join([str(rows), '-', str(count_no)])\n",
    "                        \n",
    "                        _documents.append([a, tag_doc, publish_t, authors, title])\n",
    "                        count_no+=1\n",
    "                    else:\n",
    "                        pass\n",
    "                          \n",
    "    return _documents\n",
    "def _preprocess(docs, tokens_only=False):\n",
    "    for i, rec in enumerate(docs):\n",
    "        doc = rec[0]\n",
    "        tag_doc = rec[1]\n",
    "        tag_doc = ''.join([tag_doc, '_', str(i)])\n",
    "        tokens = gensim.utils.simple_preprocess(doc)\n",
    "        if tokens_only:\n",
    "            yield tokens\n",
    "        else:\n",
    "            yield gensim.models.doc2vec.TaggedDocument(tokens, [tag_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T12:13:20.754422Z",
     "iopub.status.busy": "2021-10-31T12:13:20.754110Z",
     "iopub.status.idle": "2021-10-31T12:13:25.417014Z",
     "shell.execute_reply": "2021-10-31T12:13:25.415981Z",
     "shell.execute_reply.started": "2021-10-31T12:13:20.754383Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = final_data[['abstract', 'publish_time', 'authors', 'title']]\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "train_docs = tokenize_docs(train_data)\n",
    "df_train = pd.DataFrame.from_records(train_docs, columns = ['document', 'tag_doc', 'publish_time', 'authors', 'title'])\n",
    "df_train.drop(columns=['document'], axis=1, inplace=True)\n",
    "\n",
    "print('Trained-Documents: ', len(train_docs))\n",
    "train_cor_pus = list(_preprocess(train_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T12:13:25.419980Z",
     "iopub.status.busy": "2021-10-31T12:13:25.419680Z",
     "iopub.status.idle": "2021-10-31T12:13:26.812804Z",
     "shell.execute_reply": "2021-10-31T12:13:26.812076Z",
     "shell.execute_reply.started": "2021-10-31T12:13:25.419946Z"
    }
   },
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=1, epochs=40)\n",
    "model.build_vocab(train_cor_pus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T12:13:26.814947Z",
     "iopub.status.busy": "2021-10-31T12:13:26.814223Z",
     "iopub.status.idle": "2021-10-31T12:14:19.100906Z",
     "shell.execute_reply": "2021-10-31T12:14:19.099691Z",
     "shell.execute_reply.started": "2021-10-31T12:13:26.814912Z"
    }
   },
   "outputs": [],
   "source": [
    "model.train(train_cor_pus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User input tool function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T12:14:19.102630Z",
     "iopub.status.busy": "2021-10-31T12:14:19.102398Z",
     "iopub.status.idle": "2021-10-31T12:14:19.115611Z",
     "shell.execute_reply": "2021-10-31T12:14:19.114613Z",
     "shell.execute_reply.started": "2021-10-31T12:14:19.102600Z"
    }
   },
   "outputs": [],
   "source": [
    "def user_answer(user_query, _results):\n",
    "    df_test = pd.DataFrame(data={'abstract':[user_query]})\n",
    "    test_docs = tokenize_docs(df_test, tests=True)\n",
    "    test_corpus = list(_preprocess(test_docs, tokens_only=True))\n",
    "\n",
    "    \n",
    "    i_vector = model.infer_vector(test_corpus[0])\n",
    "    sims = model.docvecs.most_similar([i_vector], topn=len(model.docvecs))\n",
    "\n",
    "    print('TEST Docs: «{}»\\n'.format(' '.join(test_corpus[0])))\n",
    "    print(u'Similar documents as per the sentence input using doc2vec model %s:\\n' % model)\n",
    "\n",
    "    results = [(f'TOP {i}', i) for i in range(0,_results)]\n",
    "    _papers = []\n",
    "    result = []\n",
    "    for labels, i in results:\n",
    "        print('Sentence Docs: «{}»\\n'.format(' '.join(test_corpus[0])))\n",
    "        doc_split = sims[i][0].split('_')\n",
    "        tag_doc = doc_split[0]\n",
    "        doc_index = int(doc_split[1])\n",
    "        print('Index of document from the dataset', doc_index)\n",
    "        _papers.append(int(doc_split[0].split('-')[0]))\n",
    "        extract = ' '.join(map(str, train_docs[doc_index]))\n",
    "        \n",
    "        print(u'%s %s:\\n%s\\n' % (labels, sims[i], extract))\n",
    "        print('-------')\n",
    "        print(extract)\n",
    "        result.append([tag_doc, extract])\n",
    "        \n",
    "        print('='*80)\n",
    "    \n",
    "    new_dataframe = pd.DataFrame().from_records(result, columns= ['tag_doc', 'extract'])\n",
    "    return new_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "user input tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T12:16:10.877722Z",
     "iopub.status.busy": "2021-10-31T12:16:10.877288Z",
     "iopub.status.idle": "2021-10-31T12:16:18.012762Z",
     "shell.execute_reply": "2021-10-31T12:16:18.012092Z",
     "shell.execute_reply.started": "2021-10-31T12:16:10.877679Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Input the sentence or query')\n",
    "print('::::::----------------------::::::')\n",
    "user_input = input()\n",
    "print('::::::----------------------::::::')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T12:16:24.952261Z",
     "iopub.status.busy": "2021-10-31T12:16:24.951399Z",
     "iopub.status.idle": "2021-10-31T12:16:25.039713Z",
     "shell.execute_reply": "2021-10-31T12:16:25.038616Z",
     "shell.execute_reply.started": "2021-10-31T12:16:24.952215Z"
    }
   },
   "outputs": [],
   "source": [
    "dataframe_output = user_answer(user_input, 10)\n",
    "dataframe_output = dataframe_output.merge(df_train, how='left', on='tag_doc')\n",
    "dataframe_output = dataframe_output[['publish_time', 'authors', 'title', 'extract']]\n",
    "\n",
    "dataframe_output.to_csv('./result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
